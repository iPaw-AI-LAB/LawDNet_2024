{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.deep_speech import DeepSpeech\n",
    "from utils.data_processing import load_landmark_openface,compute_crop_radius\n",
    "from config.config import DINetInferenceOptions\n",
    "from models.DINet import DINet\n",
    "from config.config import DINetTrainingOptions\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import subprocess\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "from models.Gaussian_blur import Gaussian_bluring\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchlm\n",
    "from torchlm.tools import faceboxesv2\n",
    "from torchlm.models import pipnet\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "import datetime\n",
    "\n",
    "# 获取当前时间戳\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set gaussian bluring\n",
    "g_bluring_whole_image = Gaussian_bluring(radius=3,sigma=4,padding='same')\n",
    "g_bluring_mouth_region = Gaussian_bluring(radius=5,sigma=100,padding='valid')\n",
    "\n",
    "def extract_frames_from_video(video_path,save_dir):\n",
    "    videoCapture = cv2.VideoCapture(video_path)\n",
    "    fps = videoCapture.get(cv2.CAP_PROP_FPS)\n",
    "    if int(fps) != 25:\n",
    "        print('warning: the input video is not 25 fps, it would be better to trans it to 25 fps!')\n",
    "    frames = videoCapture.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    frame_height = videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    frame_width = videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    for i in range(int(frames)):\n",
    "        ret, frame = videoCapture.read()\n",
    "        result_path = os.path.join(save_dir, str(i).zfill(6) + '.jpg')\n",
    "        cv2.imwrite(result_path, frame)\n",
    "    return (int(frame_width),int(frame_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "args = [\n",
    "    '--source_channel', '3',\n",
    "    '--mouth_region_size','256',\n",
    "    '--source_video_path','./template/27-_主播说联播_出_征_东_京_.mp4', # 视频修改此处\n",
    "    '--driving_audio_path','./template/27-_主播说联播_出_征_东_京_.wav', # 音频修改此处\n",
    "    '--res_video_dir','./', # 输出视频修改此处\n",
    "    '--deepspeech_model_path','../asserts/output_graph.pb', # deepspeech 模型修改此处\n",
    "    '--pretrained_clip_DINet_path', '../asserts/clip_training_DINet_256mouth.pth', # 模型修改此处\n",
    "]\n",
    "opt = DINetInferenceOptions().parse_args(args)\n",
    "\n",
    "\n",
    "if not os.path.exists(opt.source_video_path):\n",
    "    raise ('wrong video path : {}'.format(opt.source_video_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting frames from video: ./template/27-_主播说联播_出_征_东_京_.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############################################## extract frames from source video ##############################################\n",
    "print('extracting frames from video: {}'.format(opt.source_video_path))\n",
    "video_frame_dir = opt.source_video_path.replace('.mp4', '')\n",
    "if not os.path.exists(video_frame_dir):\n",
    "    os.mkdir(video_frame_dir)\n",
    "video_size = extract_frames_from_video(opt.source_video_path,video_frame_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting deepspeech feature from : ./template/27-_主播说联播_出_征_东_京_.wav\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############################################## extract deep speech feature ##############################################\n",
    "print('extracting deepspeech feature from : {}'.format(opt.driving_audio_path))\n",
    "if not os.path.exists(opt.deepspeech_model_path):\n",
    "    raise ('pls download pretrained model of deepspeech')\n",
    "DSModel = DeepSpeech(opt.deepspeech_model_path)\n",
    "if not os.path.exists(opt.driving_audio_path):\n",
    "    raise ('wrong audio path :{}'.format(opt.driving_audio_path))\n",
    "ds_feature = DSModel.compute_audio_feature(opt.driving_audio_path)\n",
    "res_frame_length = ds_feature.shape[0]\n",
    "ds_feature_padding = np.pad(ds_feature, ((2, 2), (0, 0)), mode='edge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dengjunli/miniconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dengjunli/miniconda3/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      " 11%|█▏        | 145/1287 [00:24<03:06,  6.11it/s]"
     ]
    }
   ],
   "source": [
    "############################################## load facial landmark ##############################################\n",
    "# print('loading facial landmarks from : {}'.format(opt.source_openface_landmark_path))\n",
    "# if not os.path.exists(opt.source_openface_landmark_path):\n",
    "#     raise ('wrong facial landmark path :{}'.format(opt.source_openface_landmark_path))\n",
    "# video_landmark_data = load_landmark_openface(opt.source_openface_landmark_path).astype(np.int64)\n",
    "\n",
    "# video_landmark_data_origin = load_landmark_openface(opt.source_openface_landmark_path).astype(np.int64)\n",
    "# print(\"video_landmark_data.shape:\",video_landmark_data_origin.shape)\n",
    "# print(\"video_landmark_data[0]:\",video_landmark_data_origin[0])  # 都是整数\n",
    "\n",
    "\n",
    "#######################\n",
    "## 改为实时生成\n",
    "\n",
    "cap = cv2.VideoCapture(opt.source_video_path)\n",
    "\n",
    "# 初始化结果数组\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "video_landmark_data = np.zeros((num_frames, 68, 2))\n",
    "\n",
    "# 加载模型\n",
    "torchlm.runtime.bind(faceboxesv2(device=\"cuda\"))  # set device=\"cuda\" if you want to run with CUDA\n",
    "# set map_location=\"cuda\" if you want to run with CUDA\n",
    "torchlm.runtime.bind(\n",
    "pipnet(backbone=\"resnet18\", pretrained=True,\n",
    "        num_nb=10, num_lms=68, net_stride=32, input_size=256,\n",
    "        meanface_type=\"300w\", map_location=\"cpu\", checkpoint=None)\n",
    ") # will auto download pretrained weights from latest release if pretrained=True\n",
    "\n",
    "# 逐帧处理视频\n",
    "frame_index = 0\n",
    "# 初始化结果数组\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# 显示进度条\n",
    "for frame_index in tqdm(range(num_frames)):\n",
    "    # print(frame_index,\"帧\")\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 进行人脸关键点检测\n",
    "    landmarks, bboxes = torchlm.runtime.forward(frame)\n",
    "\n",
    "    # 将关键点结果保存到数组中\n",
    "    for i in range(len(bboxes)):\n",
    "        video_landmark_data[frame_index] = landmarks[i]\n",
    "\n",
    "    frame_index += 1\n",
    "\n",
    "# 释放资源\n",
    "cap.release()\n",
    "\n",
    "# 显示结果数组的形状\n",
    "print(\"video_landmark_data.shape \",video_landmark_data.shape)  ## 看看跟原来一样不\n",
    "# print(\"video_landmark_data[0] \",video_landmark_data[0][0][0])\n",
    "video_landmark_data = video_landmark_data.astype(np.int64)\n",
    "\n",
    "\n",
    "## 改为实时生成  ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################## align frame with driving audio ##############################################\n",
    "print('aligning frames with driving audio')\n",
    "video_frame_path_list = glob.glob(os.path.join(video_frame_dir, '*.jpg'))\n",
    "if len(video_frame_path_list) != video_landmark_data.shape[0]:\n",
    "    raise ('video frames are misaligned with detected landmarks')\n",
    "video_frame_path_list.sort()\n",
    "video_frame_path_list_cycle = video_frame_path_list + video_frame_path_list[::-1]\n",
    "video_landmark_data_cycle = np.concatenate([video_landmark_data, np.flip(video_landmark_data, 0)], 0)\n",
    "video_frame_path_list_cycle_length = len(video_frame_path_list_cycle)\n",
    "if video_frame_path_list_cycle_length >= res_frame_length:\n",
    "    res_video_frame_path_list = video_frame_path_list_cycle[:res_frame_length]\n",
    "    res_video_landmark_data = video_landmark_data_cycle[:res_frame_length, :, :]\n",
    "else:\n",
    "    divisor = res_frame_length // video_frame_path_list_cycle_length\n",
    "    remainder = res_frame_length % video_frame_path_list_cycle_length\n",
    "    res_video_frame_path_list = video_frame_path_list_cycle * divisor + video_frame_path_list_cycle[:remainder]\n",
    "    res_video_landmark_data = np.concatenate([video_landmark_data_cycle]* divisor + [video_landmark_data_cycle[:remainder, :, :]],0)\n",
    "res_video_frame_path_list_pad = [video_frame_path_list_cycle[0]] * 2 \\\n",
    "                                + res_video_frame_path_list \\\n",
    "                                + [video_frame_path_list_cycle[-1]] * 2\n",
    "res_video_landmark_data_pad = np.pad(res_video_landmark_data, ((2, 2), (0, 0), (0, 0)), mode='edge')\n",
    "assert ds_feature_padding.shape[0] == len(res_video_frame_path_list_pad) == res_video_landmark_data_pad.shape[0]\n",
    "pad_length = ds_feature_padding.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## randomly select 5 reference images ##############################################\n",
    "print('selecting five reference images')\n",
    "ref_img_list = []\n",
    "resize_w = int(opt.mouth_region_size + opt.mouth_region_size // 4)\n",
    "resize_h = int((opt.mouth_region_size // 2) * 3 + opt.mouth_region_size // 8)\n",
    "ref_index_list = random.sample(range(5, len(res_video_frame_path_list_pad) - 2), 5)\n",
    "for ref_index in tqdm(ref_index_list):\n",
    "    crop_flag,crop_radius = compute_crop_radius(video_size,res_video_landmark_data_pad[ref_index - 5:ref_index, :, :])\n",
    "    if not crop_flag:\n",
    "        raise ('our method can not handle videos with large change of facial size!!')\n",
    "    crop_radius_1_4 = crop_radius // 4\n",
    "    ref_img = cv2.imread(res_video_frame_path_list_pad[ref_index- 3])[:, :, ::-1]\n",
    "    ref_landmark = res_video_landmark_data_pad[ref_index - 3, :, :]\n",
    "    \n",
    "    ##### 报错 dengjunli\n",
    "    \n",
    "    ref_img_crop = ref_img[\n",
    "                ref_landmark[29, 1] - crop_radius:ref_landmark[29, 1] + crop_radius * 2 + crop_radius_1_4,\n",
    "                ref_landmark[33, 0] - crop_radius - crop_radius_1_4:ref_landmark[33, 0] + crop_radius +crop_radius_1_4,\n",
    "                :]\n",
    "    \n",
    "    ## 报错\n",
    "    \n",
    "    ref_img_crop = cv2.resize(ref_img_crop,(resize_w,resize_h))\n",
    "    ref_img_crop = ref_img_crop / 255.0\n",
    "    ref_img_list.append(ref_img_crop)\n",
    "ref_video_frame = np.concatenate(ref_img_list, 2)\n",
    "ref_img_tensor = torch.from_numpy(ref_video_frame).permute(2, 0, 1).unsqueeze(0).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################## load pretrained model weight ##############################################\n",
    "print('loading pretrained model from: {}'.format(opt.pretrained_clip_DINet_path))\n",
    "model = DINet(opt.source_channel, opt.ref_channel, opt.audio_channel).cuda()\n",
    "if not os.path.exists(opt.pretrained_clip_DINet_path):\n",
    "    raise ('wrong path of pretrained model weight: {}'.format(opt.pretrained_clip_DINet_path))\n",
    "state_dict = torch.load(opt.pretrained_clip_DINet_path)['state_dict']['net_g']\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:]  # remove module.\n",
    "    new_state_dict[name] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 320, 3)\n",
      "torch.Size([1, 3, 416, 320])\n",
      "tensor([[[[0.8588, 0.8588, 0.8627,  ..., 0.3882, 0.3294, 0.3059],\n",
      "          [0.8588, 0.8588, 0.8627,  ..., 0.4000, 0.3412, 0.3176],\n",
      "          [0.8549, 0.8549, 0.8588,  ..., 0.4275, 0.3686, 0.3412],\n",
      "          ...,\n",
      "          [0.2157, 0.2157, 0.2196,  ..., 0.1373, 0.1373, 0.1373],\n",
      "          [0.2157, 0.2157, 0.2196,  ..., 0.1373, 0.1373, 0.1373],\n",
      "          [0.2157, 0.2157, 0.2196,  ..., 0.1373, 0.1373, 0.1373]],\n",
      "\n",
      "         [[0.7490, 0.7490, 0.7529,  ..., 0.2902, 0.2392, 0.2157],\n",
      "          [0.7490, 0.7490, 0.7490,  ..., 0.2980, 0.2471, 0.2235],\n",
      "          [0.7451, 0.7451, 0.7490,  ..., 0.3255, 0.2706, 0.2471],\n",
      "          ...,\n",
      "          [0.2157, 0.2157, 0.2118,  ..., 0.1490, 0.1490, 0.1490],\n",
      "          [0.2157, 0.2157, 0.2118,  ..., 0.1451, 0.1451, 0.1451],\n",
      "          [0.2157, 0.2157, 0.2118,  ..., 0.1451, 0.1451, 0.1451]],\n",
      "\n",
      "         [[0.7059, 0.7059, 0.7059,  ..., 0.2627, 0.2118, 0.1922],\n",
      "          [0.7059, 0.7059, 0.7059,  ..., 0.2745, 0.2235, 0.2039],\n",
      "          [0.7020, 0.7059, 0.7059,  ..., 0.2980, 0.2471, 0.2275],\n",
      "          ...,\n",
      "          [0.3176, 0.3176, 0.3176,  ..., 0.1961, 0.1961, 0.2000],\n",
      "          [0.3176, 0.3176, 0.3176,  ..., 0.1922, 0.1961, 0.1961],\n",
      "          [0.3176, 0.3176, 0.3176,  ..., 0.1922, 0.1961, 0.1961]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(pre_frame.shape)\n",
    "print(crop_frame_tensor.shape)\n",
    "print(crop_frame_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n",
      "/home/dengjunli/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthesizing 0/1281 frame\n",
      "torch.Size([1, 3, 416, 320])\n",
      "tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          ...,\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan],\n",
      "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "zhangliang\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:3940: error: (-215:Assertion failed) func != 0 in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m     pre_frame \u001b[39m=\u001b[39m pre_frame\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy() \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m\n\u001b[1;32m     41\u001b[0m videowriter_face\u001b[39m.\u001b[39mwrite(pre_frame[:, :, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mcopy()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8))\n\u001b[0;32m---> 42\u001b[0m pre_frame_resize \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mresize(pre_frame, (crop_frame_w,crop_frame_h))\n\u001b[1;32m     43\u001b[0m frame_data[\n\u001b[1;32m     44\u001b[0m frame_landmark[\u001b[39m29\u001b[39m, \u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m crop_radius:\n\u001b[1;32m     45\u001b[0m frame_landmark[\u001b[39m29\u001b[39m, \u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m crop_radius \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[1;32m     46\u001b[0m frame_landmark[\u001b[39m33\u001b[39m, \u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m crop_radius \u001b[39m-\u001b[39m crop_radius_1_4:\n\u001b[1;32m     47\u001b[0m frame_landmark[\u001b[39m33\u001b[39m, \u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m crop_radius \u001b[39m+\u001b[39m crop_radius_1_4,\n\u001b[1;32m     48\u001b[0m :] \u001b[39m=\u001b[39m pre_frame_resize[:crop_radius \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m,:,:]\n\u001b[1;32m     49\u001b[0m videowriter\u001b[39m.\u001b[39mwrite(frame_data[:, :, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:3940: error: (-215:Assertion failed) func != 0 in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "############################################## inference frame by frame ##############################################\n",
    "if not os.path.exists(opt.res_video_dir):\n",
    "    os.mkdir(opt.res_video_dir)\n",
    "res_video_path = os.path.join(opt.res_video_dir,os.path.basename(opt.source_video_path)[:-4] + '_facial_dubbing.mp4')\n",
    "if os.path.exists(res_video_path):\n",
    "    os.remove(res_video_path)\n",
    "res_face_path = res_video_path.replace('_facial_dubbing.mp4', '_synthetic_face.mp4')\n",
    "if os.path.exists(res_face_path):\n",
    "    os.remove(res_face_path)\n",
    "videowriter = cv2.VideoWriter(res_video_path, cv2.VideoWriter_fourcc(*'XVID'), 25, video_size)\n",
    "videowriter_face = cv2.VideoWriter(res_face_path, cv2.VideoWriter_fourcc(*'XVID'), 25, (resize_w, resize_h))\n",
    "for clip_end_index in range(5, pad_length, 1):\n",
    "    print('synthesizing {}/{} frame'.format(clip_end_index - 5, pad_length - 5))\n",
    "    crop_flag, crop_radius = compute_crop_radius(video_size,res_video_landmark_data_pad[clip_end_index - 5:clip_end_index, :, :],random_scale = 1.05)\n",
    "    if not crop_flag:\n",
    "        raise ('our method can not handle videos with large change of facial size!!')\n",
    "    crop_radius_1_4 = crop_radius // 4\n",
    "    frame_data = cv2.imread(res_video_frame_path_list_pad[clip_end_index - 3])[:, :, ::-1]\n",
    "    frame_landmark = res_video_landmark_data_pad[clip_end_index - 3, :, :]\n",
    "    crop_frame_data = frame_data[\n",
    "                        frame_landmark[29, 1] - crop_radius:frame_landmark[29, 1] + crop_radius * 2 + crop_radius_1_4,\n",
    "                        frame_landmark[33, 0] - crop_radius - crop_radius_1_4:frame_landmark[33, 0] + crop_radius +crop_radius_1_4,\n",
    "                        :]\n",
    "    crop_frame_h,crop_frame_w = crop_frame_data.shape[0],crop_frame_data.shape[1]\n",
    "    crop_frame_data = cv2.resize(crop_frame_data, (resize_w,resize_h))  # [32:224, 32:224, :]\n",
    "    \n",
    "    crop_frame_data = crop_frame_data / 255.0\n",
    "    \n",
    "    # 取消嘴部区域等于0的操作\n",
    "    crop_frame_data[opt.mouth_region_size//2:opt.mouth_region_size//2 + opt.mouth_region_size,\n",
    "                    opt.mouth_region_size//8:opt.mouth_region_size//8 + opt.mouth_region_size, :] = 0\n",
    "\n",
    "    crop_frame_tensor = torch.from_numpy(crop_frame_data).float().cuda().permute(2, 0, 1).unsqueeze(0)\n",
    "    deepspeech_tensor = torch.from_numpy(ds_feature_padding[clip_end_index - 5:clip_end_index, :]).permute(1, 0).unsqueeze(0).float().cuda()\n",
    "    # 或采用自己的deepspeech_tensor\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"检查是否输入正常:\",torch.max(crop_frame_tensor),torch.max(ref_img_tensor),torch.max(deepspeech_tensor))\n",
    "        pre_frame = model(crop_frame_tensor, ref_img_tensor, deepspeech_tensor)\n",
    "        print(pre_frame.shape)\n",
    "        print(pre_frame)\n",
    "        print(\"zhangliang\")\n",
    "        pre_frame = pre_frame.squeeze(0).permute(1, 2, 0).detach().cpu().numpy() * 255\n",
    "    videowriter_face.write(pre_frame[:, :, ::-1].copy().astype(np.uint8))\n",
    "    pre_frame_resize = cv2.resize(pre_frame, (crop_frame_w,crop_frame_h))\n",
    "    frame_data[\n",
    "    frame_landmark[29, 1] - crop_radius:\n",
    "    frame_landmark[29, 1] + crop_radius * 2,\n",
    "    frame_landmark[33, 0] - crop_radius - crop_radius_1_4:\n",
    "    frame_landmark[33, 0] + crop_radius + crop_radius_1_4,\n",
    "    :] = pre_frame_resize[:crop_radius * 3,:,:]\n",
    "    videowriter.write(frame_data[:, :, ::-1])\n",
    "videowriter.release()\n",
    "videowriter_face.release()\n",
    "video_add_audio_path = res_video_path.replace('.mp4', '_{}_add_audio.mp4'.format(timestamp))\n",
    "if os.path.exists(video_add_audio_path):\n",
    "    os.remove(video_add_audio_path)\n",
    "cmd = 'ffmpeg -i {} -i {} -c:v copy -c:a aac -strict experimental -map 0:v:0 -map 1:a:0 {}'.format(\n",
    "    res_video_path,\n",
    "    opt.driving_audio_path,\n",
    "    video_add_audio_path)\n",
    "subprocess.call(cmd, shell=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
