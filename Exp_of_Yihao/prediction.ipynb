{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import torchlm\n",
    "# from torchlm.tools import faceboxesv2\n",
    "# from torchlm.models import pipnet\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.LawDNet_new import LawDNet \n",
    "from collections import OrderedDict\n",
    "import warnings \n",
    "# from cfg import *\n",
    "# from tools import *\n",
    "\n",
    "from torchlm.tools import faceboxesv2\n",
    "from torchlm.models import pipnet\n",
    "\n",
    "# from torch_affine_ops import *\n",
    "from tensor_processing import *\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_np(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    H, W = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    print('video length: ', length)\n",
    "    print('video size: ','H--', H, 'W--',W)\n",
    "    \n",
    "\n",
    "    frames = []\n",
    "    frames_id = 1\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret and frames_id <= 1500:\n",
    "            frames_id += 1 \n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '../template/1-坐.mp4'\n",
    "video_frames = read_video_np(video_path)\n",
    "video_frames = np.array(video_frames, dtype=np.float32)\n",
    "video_frames = video_frames[:, :, :, ::-1] # BGR to RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio\n",
    "deepspeech_tensor = torch.load('../template/longaudio_deepspeech_tensor_all.pt')\n",
    "B = 10\n",
    "len_out = (np.min([len(video_frames), deepspeech_tensor.shape[0]])//B)*B\n",
    "video_frames = video_frames[:len_out]\n",
    "deepspeech_tensor = deepspeech_tensor[:len_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "net_g = LawDNet(3,15,29).to(device)\n",
    "checkpoint = torch.load('/home/hermit_delta/data/hermit_delta/DINet-update/checkpoints/实验11-smoothsqmask-netG_model_epoch_25.pth') \n",
    "state_dict = checkpoint['state_dict']['net_g']\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:]  # remove module.\n",
    "    new_state_dict[name] = v\n",
    "net_g.load_state_dict(new_state_dict)\n",
    "net_g.eval()\n",
    "facealigner = FaceAlign(ratio=1.6, device=device)\n",
    "sqmasker = SmoothSqMask()\n",
    "#sqmasker = SmoothSqMask(radius=30,sigma=10,standard_shape=(416,320),padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### landmark\n",
    "torchlm.runtime.bind(faceboxesv2(device=device))  # set device=\"cuda\" if you want to run with CUDA\n",
    "# set map_location=\"cuda\" if you want to run with CUDA\n",
    "torchlm.runtime.bind(\n",
    "  pipnet(backbone=\"resnet18\", pretrained=True,  \n",
    "         num_nb=10, num_lms=68, net_stride=32, input_size=256,\n",
    "         meanface_type=\"300w\", map_location=device.__str__(), checkpoint=None) \n",
    ") # will auto download pretrained weights from latest release if pretrained=True\n",
    "landmarks,_ = torchlm.runtime.forward(video_frames[0])\n",
    "\n",
    "\n",
    "landmarks_list = []\n",
    "for i in range(len(video_frames)):\n",
    "    landmarks, bboxes = torchlm.runtime.forward(video_frames[i])\n",
    "    trust = bboxes[:,4]\n",
    "    faithful_index = np.argsort(trust)[-1]\n",
    "    landmarks_list.append(landmarks[faithful_index,:,:])\n",
    "\n",
    "\n",
    "landmarks_list = np.array(landmarks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform reference from random 5 frames\n",
    "\n",
    "reference_index = torch.randint(0, len(video_frames), (5,)).tolist()\n",
    "\n",
    "reference_tensor = torch.from_numpy(video_frames[reference_index]).to(device)\n",
    "reference_tensor = reference_tensor.permute(0,3,1,2)\n",
    "reference_landmarks = torch.from_numpy(landmarks_list[reference_index]).to(device)\n",
    "\n",
    "reference_tensor,_,_ = facealigner(reference_tensor, reference_landmarks,out_W=320)\n",
    "\n",
    "plt.imshow(reference_tensor[0].permute(1,2,0).cpu().numpy().astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference(model,masked_source,reference,audio_tensor):\n",
    "    with torch.no_grad():\n",
    "        output = model(masked_source,reference,audio_tensor)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outframes = np.zeros_like(video_frames)\n",
    "\n",
    "for i in range(len_out//B):\n",
    "    source_tensor = torch.from_numpy(video_frames[i*B:(i+1)*B].copy()).to(device)\n",
    "    source_tensor = source_tensor.permute(0,3,1,2)\n",
    "    landmarks_tensor = torch.from_numpy(landmarks_list[i*B:(i+1)*B]).to(device)\n",
    "    feed_tensor,_,affine_matrix = facealigner(source_tensor, landmarks_tensor,out_W=320)\n",
    "    _,C,H,W = feed_tensor.shape\n",
    "\n",
    "    feed_tensor_masked = sqmasker(feed_tensor)\n",
    "\n",
    "    reference_tensor_B = feed_tensor[2:7].view(1,5*3,H,W).repeat(B,1,1,1)\n",
    "\n",
    "    #reference_tensor_B = reference_tensor.unsqueeze(0).repeat(B,1,1,1,1).view(B,5*3,H,W)\n",
    "\n",
    "    audio_tensor = deepspeech_tensor[i*B:(i+1)*B].to(device)\n",
    "\n",
    "    output_B = reference(net_g,feed_tensor_masked/255.0,reference_tensor_B/255.0,audio_tensor)\n",
    "    # covert half into float\n",
    "    output_B = output_B.float().clamp_(0,1)*255.0\n",
    "    \n",
    "    # feathering\n",
    "    output_B = facealigner.feathering(output_B,feed_tensor)\n",
    "    # recover\n",
    "    outframes_B = facealigner.recover(output_B,source_tensor,affine_matrix)\n",
    "\n",
    "    outframes_B = outframes_B.permute(0,2,3,1).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "    outframes[i*B:(i+1)*B] = outframes_B\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outframes = outframes.astype(np.uint8) \n",
    "outframes = outframes[:,:,:,::-1] # RGB to BGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "videoWriter = cv2.VideoWriter('output.mp4', fourcc, 25, (video_frames.shape[2],video_frames.shape[1]),True)\n",
    "for i in range(len_out):\n",
    "    videoWriter.write(outframes[i])\n",
    "videoWriter.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(outframes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(feed_tensor[0].permute(1,2,0).cpu().numpy().astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from ffmpy import FFmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 视频添加音频\n",
    "def video_add_audio(video_path: str, audio_path: str, output_dir: str):\n",
    "    _ext_video = os.path.basename(video_path).strip().split('.')[-1]\n",
    "    _ext_audio = os.path.basename(audio_path).strip().split('.')[-1]\n",
    "    if _ext_audio not in ['mp3', 'wav']:\n",
    "        raise Exception('audio format not support')\n",
    "    _codec = 'copy'\n",
    "    if _ext_audio == 'wav':\n",
    "        _codec = 'aac'\n",
    "    result = os.path.join(\n",
    "        output_dir, '{}.{}'.format(\n",
    "            uuid.uuid4(), _ext_video))\n",
    "    ff = FFmpeg(\n",
    "        inputs={video_path: None, audio_path: None},\n",
    "        outputs={result: '-map 0:v -map 1:a -c:v copy -c:a {} -shortest'.format(_codec)})\n",
    "    print(ff.cmd)\n",
    "    ff.run()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_add_audio('output.mp4', '../template/RD_Radio14_000_corrected.wav', './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Human3D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
